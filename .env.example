# ==================== Web Scraper v0.0.2 Configuration ====================
# Copy this file to .env and update with your values
# cp .env.example .env

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

APP_NAME=Web Scraper
APP_VERSION=0.0.2
API_PREFIX=/api/v1
ENVIRONMENT=development

# ============================================================================
# SCRAPER SETTINGS
# ============================================================================

# Maximum number of pages to scrape concurrently (1-20)
# Lower = safer, Higher = faster but more resource intensive
MAX_CONCURRENT_PAGES=5

# Page load timeout in milliseconds (10000-60000)
PAGE_TIMEOUT=30000

# Maximum crawl depth (1-10)
# How many levels deep to follow links from the starting URL
MAX_DEPTH=5

# Respect robots.txt directives (true/false)
RESPECT_ROBOTS_TXT=true

# User agent string
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# ============================================================================
# STORAGE SETTINGS
# ============================================================================

# Base directory for scraped data
# Files saved as: OUTPUT_DIR/{domain}_{timestamp}/
# Contains: sitemap.json, pages.json, pages.csv, summary.json
OUTPUT_DIR=./scraped_data

# ============================================================================
# SECURITY SETTINGS
# ============================================================================

# Allowed domains (comma-separated, leave empty to allow all)
ALLOWED_DOMAINS=

# Minimum authorization token length
MIN_TOKEN_LENGTH=10

# CORS allowed origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

# Maximum number of active jobs in memory
MAX_JOBS_IN_MEMORY=100

# Job cleanup interval in seconds
JOB_CLEANUP_INTERVAL=3600

# Maximum job age in seconds before cleanup
MAX_JOB_AGE=86400

# ============================================================================
# LOGGING SETTINGS
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable console logging (true/false)
LOG_TO_CONSOLE=true

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================

# Browser headless mode (true/false)
BROWSER_HEADLESS=true

# Enable JavaScript (true/false)
ENABLE_JAVASCRIPT=true

# Wait for network idle (true/false)
WAIT_FOR_NETWORK_IDLE=true

# Maximum retries for failed pages
MAX_RETRIES=3

# Retry delay in milliseconds
RETRY_DELAY=1000

# ============================================================================
# DEVELOPMENT SETTINGS
# ============================================================================

DEBUG=false
HOT_RELOAD=true
DEV_PORT=8000
DEV_HOST=0.0.0.0

# ============================================================================
# NOTES
# ============================================================================

# v2.0 Changes:
# - No image downloads (URLs only)
# - Hierarchical sitemap building
# - JSON + CSV output formats
# - Persistent job storage
# - Smart URL filtering (skips .pdf, .png, .csv, etc.)
# - Same-domain restriction

# Recommended Settings:

# Development (Fast testing):
# MAX_CONCURRENT_PAGES=10
# PAGE_TIMEOUT=20000
# MAX_DEPTH=2

# Production (Stable):
# MAX_CONCURRENT_PAGES=3
# PAGE_TIMEOUT=30000
# MAX_DEPTH=5

# Deep Crawl (Comprehensive):
# MAX_CONCURRENT_PAGES=5
# PAGE_TIMEOUT=40000
# MAX_DEPTH=8