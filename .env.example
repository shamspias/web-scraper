# ==================== SAVE AS: backend/.env.example ====================
# Copy this file to .env and update with your values
# cp .env.example .env

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Application name
APP_NAME=Authorized Website Scraper

# Application version
APP_VERSION=1.0.0

# API prefix (e.g., /api/v1)
API_PREFIX=/api/v1

# Environment (development, production, staging)
ENVIRONMENT=development

# ============================================================================
# SCRAPER SETTINGS
# ============================================================================

# Maximum number of pages to scrape concurrently (1-20)
# Lower = safer, Higher = faster but more resource intensive
MAX_CONCURRENT_PAGES=5

# Page load timeout in milliseconds (10000-60000)
# How long to wait for a page to load before giving up
PAGE_TIMEOUT=30000

# Maximum crawl depth (1-10)
# How many levels deep to follow links from the starting URL
MAX_DEPTH=5

# Respect robots.txt directives (true/false)
RESPECT_ROBOTS_TXT=true

# User agent string for web requests
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# ============================================================================
# STORAGE SETTINGS
# ============================================================================

# Base directory for scraped data
# Files will be saved to: OUTPUT_DIR/{domain}_{timestamp}/
OUTPUT_DIR=./scraped_data

# Whether to download and save images (true/false)
SAVE_IMAGES=true

# Maximum images to download per page (0-100, 0 = unlimited)
MAX_IMAGES_PER_PAGE=30

# ============================================================================
# SECURITY SETTINGS
# ============================================================================

# Allowed domains (comma-separated, leave empty to allow all)
# Example: example.com,test.com,mysite.org
ALLOWED_DOMAINS=

# Minimum authorization token length
MIN_TOKEN_LENGTH=10

# Enable/disable API key authentication (true/false)
REQUIRE_API_KEY=false

# API key (only if REQUIRE_API_KEY=true)
API_KEY=

# CORS allowed origins (comma-separated)
# For development with separate frontend: http://localhost:3000
# For production: your-domain.com
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

# Maximum number of active jobs to keep in memory
MAX_JOBS_IN_MEMORY=100

# Job cleanup interval in seconds (how often to clean old jobs)
JOB_CLEANUP_INTERVAL=3600

# Maximum job age in seconds before auto-cleanup
MAX_JOB_AGE=86400

# ============================================================================
# LOGGING SETTINGS
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log file path (leave empty to disable file logging)
LOG_FILE=./logs/scraper.log

# Enable/disable console logging (true/false)
LOG_TO_CONSOLE=true

# ============================================================================
# DATABASE SETTINGS (Optional - for future use)
# ============================================================================

# Database URL (for job persistence)
# Leave empty to use in-memory storage
# Example: postgresql://user:password@localhost:5432/dbname
DATABASE_URL=

# Redis URL (for job queue)
# Leave empty to use in-memory queue
# Example: redis://localhost:6379/0
REDIS_URL=

# ============================================================================
# RATE LIMITING (Optional)
# ============================================================================

# Enable rate limiting (true/false)
ENABLE_RATE_LIMITING=false

# Rate limit: requests per minute per IP
RATE_LIMIT_PER_MINUTE=60

# ============================================================================
# MONITORING & ALERTS (Optional)
# ============================================================================

# Sentry DSN for error tracking (leave empty to disable)
SENTRY_DSN=

# Enable metrics collection (true/false)
ENABLE_METRICS=false

# Webhook URL for job completion notifications (leave empty to disable)
WEBHOOK_URL=

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================

# Browser headless mode (true/false)
# Set to false for debugging to see browser window
BROWSER_HEADLESS=true

# Enable JavaScript (true/false)
ENABLE_JAVASCRIPT=true

# Wait for network idle before scraping (true/false)
WAIT_FOR_NETWORK_IDLE=true

# Screenshot pages (true/false)
TAKE_SCREENSHOTS=false

# Maximum retries for failed page loads
MAX_RETRIES=3

# Delay between retries in milliseconds
RETRY_DELAY=1000

# ============================================================================
# DEVELOPMENT SETTINGS
# ============================================================================

# Enable debug mode (true/false)
DEBUG=false

# Enable hot reload (true/false)
HOT_RELOAD=true

# Port for development server
DEV_PORT=8000

# Host for development server
DEV_HOST=0.0.0.0

# ============================================================================
# NOTES
# ============================================================================

# 1. Never commit .env file to version control
# 2. Use strong, unique tokens for production
# 3. Adjust MAX_CONCURRENT_PAGES based on your server resources
# 4. Lower PAGE_TIMEOUT for faster failures on slow sites
# 5. Set SAVE_IMAGES=false to save disk space if images aren't needed
# 6. Use ALLOWED_DOMAINS in production to restrict scraping
# 7. Enable rate limiting in production to prevent abuse
# 8. Set LOG_LEVEL=ERROR in production for better performance

# ============================================================================
# EXAMPLE CONFIGURATIONS
# ============================================================================

# Development (Fast, verbose logging):
# MAX_CONCURRENT_PAGES=10
# PAGE_TIMEOUT=20000
# LOG_LEVEL=DEBUG
# SAVE_IMAGES=false

# Production (Conservative, stable):
# MAX_CONCURRENT_PAGES=3
# PAGE_TIMEOUT=30000
# LOG_LEVEL=WARNING
# SAVE_IMAGES=true
# ENABLE_RATE_LIMITING=true

# High-performance (Fast, resource-intensive):
# MAX_CONCURRENT_PAGES=20
# PAGE_TIMEOUT=15000
# MAX_DEPTH=10
# SAVE_IMAGES=false